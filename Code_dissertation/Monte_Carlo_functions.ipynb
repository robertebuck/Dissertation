{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from misc_functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import misc_functions as msc_fct\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_orders(dt, Ndt, NSIM, kappa, lamb):\n",
    "    '''\n",
    "    Simulates the arrival of market buy orders.\n",
    "    \n",
    "    imputs:\n",
    "    dt: time step\n",
    "    Ndt: number of time steps\n",
    "    NSIM: number of simmulations\n",
    "    kappa: intensity of exponential distribution\n",
    "    lamb: intensity of poission process\n",
    "    \n",
    "    returns:\n",
    "    sim: 2D numpy array of depths reached by buy orders for\n",
    "    each simulation and time period\n",
    "    '''\n",
    "    \n",
    "    # pre processing various matricies\n",
    "    sim = np.full((NSIM,Ndt), np.nan)\n",
    "    rand = np.random.rand(NSIM, Ndt)\n",
    "    \n",
    "    # giving depth reached by buy orders\n",
    "    exp = np.random.exponential(1/kappa,(NSIM,Ndt))\n",
    "    \n",
    "    # probability of buy order arriving\n",
    "    P = 1 - np.exp(-lamb*dt)\n",
    "    \n",
    "    # boolian array 1 if order arrives\n",
    "    arrive = np.less(rand, P)\n",
    "    \n",
    "    for k in range(0,NSIM):\n",
    "        idx = arrive[k,:]\n",
    "        sim[k,idx] = exp[k,idx]\n",
    "    \n",
    "    # Adding a collumn of nan for t=0\n",
    "    sim = np.insert(sim, 0, np.nan, axis=1)\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretisation(sim, sections):\n",
    "    '''\n",
    "    Splits up the depths reached by buy orders depending on \n",
    "    the frequency of arrival\n",
    "    \n",
    "    imput:\n",
    "    sim: 2D numpy array of depths reached by buy orders\n",
    "    sections: how many desired sections to split the depth\n",
    "    into\n",
    "    \n",
    "    output:\n",
    "    bins: cut off of each section\n",
    "    mid: midpoint of each section\n",
    "    '''\n",
    "    \n",
    "    # turns 2d numpy array into 1 d array\n",
    "    sim_1d = sim.flatten()\n",
    "    sim_1d = pd.DataFrame(sim_1d)\n",
    "    \n",
    "    # splits in order to have even fequency in each bin\n",
    "    sim_1d = pd.qcut(sim_1d.loc[:,0], q=(sections), labels=False, retbins = True)\n",
    "    bins = sim_1d[1]\n",
    "    \n",
    "    mid = np.full((bins.size-1), np.nan)\n",
    "    \n",
    "    # producing mid value of bins\n",
    "    for k in range(1, bins.size, 1):\n",
    "        mid[k-1] = (bins[k-1] + bins[k])/2\n",
    "    \n",
    "    return bins, mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action(t, q, Ndt, Nq, mid):\n",
    "    '''\n",
    "    Produces a dictionary of all state action pairs\n",
    "    \n",
    "    imput:\n",
    "    t: array of times\n",
    "    q: range of quantities of inventory\n",
    "    Ndt: number of time steps\n",
    "    Nq: starting inventory\n",
    "    mid: range of depths that limit orders can be posted\n",
    "    \n",
    "    output:\n",
    "    state_actions_pairs: dictonary with state values as keys\n",
    "    and actions as values\n",
    "    '''\n",
    "    \n",
    "    states = []\n",
    "    \n",
    "    \n",
    "    # loop to get all possible states\n",
    "    for k in range(1,Ndt+1,1):\n",
    "        for i in range(0,Nq+1,1):\n",
    "            state = (t[k],q[i])    \n",
    "            states.append(state)\n",
    "            \n",
    "    state_actions_pairs={}\n",
    "    \n",
    "    # loop to add actions to states\n",
    "    for k in range(0,len(states),1):\n",
    "        if states[k][1]==0:\n",
    "            state_actions_pairs[states[k]] = [(0,False)] # (a,b) a - market orders, b - depths of limit orders\n",
    "        else:\n",
    "            actions = []\n",
    "            for i in range(1,states[k][1]+2,1):\n",
    "                if i-1 != states[k][1]:\n",
    "                    for m in range(0,len(mid),1):\n",
    "                        actions.append((i-1,mid[m]))\n",
    "                else:\n",
    "                    actions.append((i-1,False))\n",
    "                    \n",
    "            state_actions_pairs[states[k]] = actions\n",
    "            \n",
    "    return state_actions_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(policy, simulation, S0, Nq, Ndt, dt, xi, sigma):\n",
    "    '''\n",
    "    Runs a given policy\n",
    "    \n",
    "    imputs:\n",
    "    policy: dictionary contaning the policy to be run\n",
    "    simulation: the simulated buy orders reccieved\n",
    "    S0: inital mid price\n",
    "    Nq: inital number of inventory to be liquidated\n",
    "    Ndt: the number of time steps\n",
    "    dt: time step size\n",
    "    xi: half spread\n",
    "    sigma: variation of midprice\n",
    "    \n",
    "    output:\n",
    "    X: terminal value of wealth process\n",
    "    Q: inventory trajectory\n",
    "    '''\n",
    "    \n",
    "    # ensuring when market buy order arives this is denonted by - inf\n",
    "    simulation = msc_fct.nan_overwrite(simulation, -np.inf)\n",
    "    \n",
    "    # preconditioning\n",
    "    Q = np.full(Ndt+1,np.nan)\n",
    "    Q[0] = Nq\n",
    "    S = S0   \n",
    "    X = 0\n",
    "    \n",
    "    #running over all time steps\n",
    "    for i in range(1,Ndt+1,1):\n",
    "        #selecting given policy for each time step\n",
    "        action = policy[(i*dt, Q[i-1])]\n",
    "        #If limit order is placed\n",
    "        if action[1] != False:\n",
    "            #if filled\n",
    "            if simulation[i] > action[1]:\n",
    "                X = X + (S - xi) * action[0] + (S + action[1])\n",
    "                Q[i] = Q[i-1] - action[0] - 1\n",
    "            #if not filled    \n",
    "            else:\n",
    "                X = X + (S - xi) * action[0]\n",
    "                Q[i] = Q[i-1] - action[0]\n",
    "        #if no limit order placed        \n",
    "        else:\n",
    "            X = X + (S - xi) * action[0]\n",
    "            Q[i] = Q[i-1] - action[0]\n",
    "            S = S + sigma*np.random.normal(1)\n",
    "    #if terminal inventory remains liquidate it        \n",
    "    if Q[-1] != 0:\n",
    "        X = X + (S - xi)*Q[-1]\n",
    "        Q[-1] = 0\n",
    "    return X, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X, Q, q_target, phi, Ndt, dt):\n",
    "    '''\n",
    "    Computes the performance  for a sample outupt\n",
    "    \n",
    "    imputs:\n",
    "    X: terminal value of wealth process\n",
    "    Q: trajectory of inventory\n",
    "    q_target: target inventory trajectory\n",
    "    phi: weight placed on deviations from inventory target\n",
    "    Ndt: number of time steps\n",
    "    dt: time step\n",
    "    \n",
    "    output:\n",
    "    perf: performace value\n",
    "    '''\n",
    "    \n",
    "    if q_target[0] != None:\n",
    "        perf = X - phi*(np.cumsum(np.power(Q-q_target,2)*dt))[Ndt]\n",
    "    else:\n",
    "        perf = X\n",
    "    \n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_policy(state_actions, Ndt, dt, Nq):\n",
    "    '''\n",
    "    Produces a random policy\n",
    "    \n",
    "    imputs:\n",
    "    state_actions: all possible state action pairs\n",
    "    Ndt: number of time steps\n",
    "    dt: time step size\n",
    "    Nq: number of units of inventory\n",
    "    \n",
    "    outputs:\n",
    "    policy: a random policy\n",
    "    '''\n",
    "    policy = {}\n",
    "\n",
    "    # getting random state action pairs for all other possible states and actions\n",
    "    for i in range(1, Ndt+1, 1):\n",
    "        for k in range(0, Nq + 1, 1):\n",
    "            policy[(i*dt, k)] = random.choice(state_actions[(i*dt, k)])\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policies(policy, state_actions, Ndt, dt, Nq, confidence):\n",
    "    ''' \n",
    "    Returns an array of possible policies in a format that can be used in\n",
    "    the optimum policy\n",
    "    \n",
    "    imput:\n",
    "    policy: the inital policy that will be used to produce other policies\n",
    "    state_actions: all possible states and actions\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    policy_values = np.array([policy, 0, 0, 0])\n",
    "    policy_values.shape = (1,4)\n",
    "    p = dict(policy)\n",
    "    m = 0\n",
    "    \n",
    "    while m < confidence:\n",
    "        p = dict(policy)\n",
    "        p[(dt, Nq)] = random.choice(state_actions[(dt, Nq)])\n",
    "        \n",
    "        for i in range(2, Ndt + 1, 1):\n",
    "        \n",
    "            for k in range(0, Nq+1, 1):\n",
    "                action = random.choice(state_actions[(i*dt, k)])\n",
    "                p[(i*dt, k)] = action\n",
    "    \n",
    "        idx = policy_values[:,0] == p\n",
    "        if sum(idx) == 0:\n",
    "            policy_values = np.vstack((policy_values, np.array([p,0,0,0])))\n",
    "        \n",
    "        m = m + 1\n",
    "    \n",
    "    return policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimum_policy(policy_values, epsilon, state_actions, Ndt, dt, Nq, S0, sigma, xi, sim, q_target, phi, conf, comp):\n",
    "    '''\n",
    "    Produces an optimal policy\n",
    "    \n",
    "    imputs:\n",
    "    policy_values: the policy values\n",
    "    state_actions: state action pairs\n",
    "    Ndt: number of time steps\n",
    "    dt: time step size\n",
    "    Nq: number of units of inventory\n",
    "    S0: inital midprice price\n",
    "    sigma: volatility of midprice\n",
    "    xi: half bid ask spread i.e. cost of executing market orders\n",
    "    sim: data points\n",
    "    q_target: target liquidation schedule\n",
    "    phi: parameter effecting how strongly agent penalised for deviating from target liquidation schedule\n",
    "    conf: minimum number of times to run each policy\n",
    "    comp: number of policies to compare optimal policy to\n",
    "    \n",
    "    outputs:\n",
    "    pol: optimal policy\n",
    "    value: performance criteria for optimal policy\n",
    "    N: number of times optimal policy is run\n",
    "    t1: time taken to find optimal policy\n",
    "    res: policy, performance criteria and number of times run for each of the policies being compared to the optimal one\n",
    "    '''\n",
    "    t0 = time.clock()\n",
    "    \n",
    "    sim = msc_fct.nan_overwrite(sim, -np.inf)\n",
    "    Q = np.full(Ndt+1, np.nan)\n",
    "    Q[0] = Nq\n",
    "    S = S0\n",
    "    X = 0\n",
    "    \n",
    "    for m in range(0, len(policy_values[:,1]), 1):\n",
    "        for k in range(0, conf, 1):\n",
    "            simulation = sim[k,:]\n",
    "            Q = np.full(Ndt + 1, np.nan)\n",
    "            Q[0] = Nq\n",
    "            S = S0\n",
    "            X = 0\n",
    "            policy = policy_values[m,0]\n",
    "            X, Q = run_policy(policy, simulation, S0, Nq, Ndt, dt, xi, sigma)\n",
    "            V = performance(X, Q, q_target, phi, Ndt, dt)\n",
    "            idx = policy_values[:,0] == policy\n",
    "            \n",
    "            if k == 0:    \n",
    "                policy_values[idx,1] = policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V\n",
    "            \n",
    "                policy_values[idx,2] = policy_values[idx,2] + 1 \n",
    "            else:\n",
    "                policy_values[idx,3] = policy_values[idx,3] + (V - policy_values[idx,1])*(V - policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V)\n",
    "                \n",
    "                policy_values[idx,1] = policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V\n",
    "                \n",
    "                policy_values[idx,2] = policy_values[idx,2] + 1 \n",
    "\n",
    "\n",
    "    for k in range(conf + 1, len(sim[:,0]), 1):\n",
    "        \n",
    "        z = 0\n",
    "    \n",
    "        while policy_values[np.where(policy_values[:,2] == np.amin(policy_values[:,2])),2][0,0] < conf:\n",
    "        \n",
    "            simulation = sim[z,:]\n",
    "            Q = np.full(Ndt + 1, np.nan)\n",
    "            Q[0] = Nq\n",
    "            S = S0\n",
    "            X = 0\n",
    "        \n",
    "            policy = policy_values[np.where(policy_values[:,2] == np.amin(policy_values[:,2])),0][0,0]\n",
    "            X, Q = run_policy(policy, simulation, S0, Nq, Ndt, dt, xi, sigma)\n",
    "            V = performance(X, Q, q_target, phi, Ndt, dt)\n",
    "            \n",
    "            idx = policy_values[:,0] == policy\n",
    "            \n",
    "            if z == 0:    \n",
    "                policy_values[idx,1] = policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V\n",
    "            \n",
    "                policy_values[idx,2] = policy_values[idx,2] + 1 \n",
    "            else:\n",
    "                policy_values[idx,3] = policy_values[idx,3] + (V - policy_values[idx,1])*(V - policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V)\n",
    "                \n",
    "                policy_values[idx,1] = policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "                (1/(policy_values[idx,2] + 1))*V\n",
    "                \n",
    "                policy_values[idx,2] = policy_values[idx,2] + 1 \n",
    "            \n",
    "            z = z + 1\n",
    "        \n",
    "       \n",
    "    \n",
    "        simulation = sim[k,:]\n",
    "        Q[0] = Nq\n",
    "        S = S0\n",
    "        X = 0\n",
    "    \n",
    "        policy = policy_values[np.where(policy_values[:,1] == np.amax(policy_values[:,1])),0][0,0]\n",
    "        p = dict(policy)  \n",
    "    \n",
    "        for i in range(1, Ndt+1, 1):\n",
    "            #selecting given action for each time step via epsilion greedy approach\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                action = random.choice(state_actions[(i*dt, Q[i-1])])\n",
    "            else:\n",
    "                action = policy[(i*dt, Q[i-1])]\n",
    "            \n",
    "            p[(i*dt, Q[i-1])] = action\n",
    "    \n",
    "            #If limit order is placed\n",
    "            if action[1] != False:\n",
    "                #if filled\n",
    "                if simulation[i] > action[1]:\n",
    "                    X = X + (S - xi) * action[0] + (S + action[1])\n",
    "                    Q[i] = Q[i-1] - action[0] - 1\n",
    "                #if not filled    \n",
    "                else:\n",
    "                    X = X + (S - xi) * action[0]\n",
    "                    Q[i] = Q[i-1] - action[0]\n",
    "            #if no limit order placed        \n",
    "            else:\n",
    "                X = X + (S - xi) * action[0]\n",
    "                Q[i] = Q[i-1] - action[0]\n",
    "                S = S + sigma*np.random.normal(1)\n",
    "        #if terminal inventory remains liquidate it        \n",
    "        if Q[-1] != 0:\n",
    "            X = X + (S - xi)*Q[-1]\n",
    "            Q[-1] = 0\n",
    "        \n",
    "        V = performance(X, Q, q_target, phi, Ndt, dt)\n",
    "\n",
    "        idx = policy_values[:,0] == p\n",
    "        \n",
    "        if sum(idx) > 0:\n",
    "            policy_values[idx,3] = policy_values[idx,3] + (V - policy_values[idx,1])*(V - policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "            (1/(policy_values[idx,2] + 1))*V)\n",
    "                \n",
    "            policy_values[idx,1] = policy_values[idx,1]*(policy_values[idx,2]/(policy_values[idx,2] + 1)) + \\\n",
    "            (1/(policy_values[idx,2] + 1))*V\n",
    "                \n",
    "            policy_values[idx,2] = policy_values[idx,2] + 1\n",
    "            \n",
    "        else:\n",
    "            policy_values = np.vstack((policy_values, np.array([p,V,1,0])))\n",
    "    \n",
    "    \n",
    "    pol = policy_values[np.where(policy_values[:,1] == np.amax(policy_values[:,1])),0][0,0]\n",
    "    value = policy_values[np.where(policy_values[:,1] == np.amax(policy_values[:,1])),1][0,0]\n",
    "    N = policy_values[np.where(policy_values[:,1] == np.amax(policy_values[:,1])),2][0,0]\n",
    "        \n",
    "    t1 = time.clock() - t0\n",
    "    \n",
    "    comparision = np.partition(policy_values[:,1], comp)[-comp:]\n",
    "    \n",
    "    res = np.full((comp,3),np.nan)\n",
    "    for i in range(0,comp,1):\n",
    "        res[i,0] = policy_values[np.where(policy_values[:,1] == comparision[i]),1][0,0]\n",
    "        res[i,1] = policy_values[np.where(policy_values[:,1] == comparision[i]),2][0,0]\n",
    "        res[i,2] = policy_values[np.where(policy_values[:,1] == comparision[i]),3][0,0]\n",
    "    \n",
    "    return pol, value, N, t1, res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
